Backend & API Caching
=====================

*DO NOTE*
Caching is only Optional. System should work with/without caching by flipping USE_CACHE in local_settings.py

Background
----------

Until the previous release there was a central cache server which would serve Cache + Nodding to

    1. Blackjack
    2. Party
    3. API
    4. Timber

Drawbacks:

    * This serves only until all servers live on the same Machine.
    * Added responsibility on API of invalidating the Common Cache.
    * Caching was done with the Aim of avodiding API Calls, which meant only macro calls were cached.
    * Effectively, Only Authtoken & Siminar Get was cached on Blackjack.
    * API Bouncer still went through a brunt of heavy I/O & Bouncer is called for almost all API calls.

Changes
--------

Starting with this release, each Server is responsible for maintaining its own cache.
This allows API to aggresively cache stuff & lighten each Bouncer call which involves Siminar/Topic.
Resources that would be Cached:

    1. Demand Cache for topic_get in Bouncer [Expiration Policy: Evented]
    2. Demand Cache for Authtoken. [Expiration Policy: Evented]
    3. Spatial Cache for Search Queries. [Expiration Policy: TTL]


TopicGet
--------

While Bundle and Session API calls are lighter, Siminar GET computes oldest_step, curated_items, todo_items for each step.
This can be avoided if API would cache the topic per user.

With an **Exception** that every time topic requires fields other than the default fields, Cache should not be engaged.
This ensures that we dont cache the full siminar view which is used in Siminar setup pages.

Authtoken
---------

Starting with the October release, User returned with Authotoken GET contains two important keys:

    * bundles: `{"bundles_dict": {}, "facilitating": [], "participating": [], "unlaunched": []}`
    * roles: `{"siminars": {}, "participating": [], "facilitating": [], "unlaunched": []}`

`bundles` and `roles` keys act as User scope and prevent the need of back-and-forth context calls from the API.

Authtoken in DB is saved as

```
authtoken = {
    _id: 138097802129319459855057
    created_on: 1380978021293L,
    modified_by: 129328102413829737863702
    modified_on: 1382581077346L,
    user: {
        _id: 138069174058727688663702
        display_name: t t
        email: t@t.com
        is_validated: True,
        relation: None,
        socket_url: 127.0.0.1:8090
        timezone: Asia/Kolkata
    },
    user_id: 138069174058727688663702
}
```
where user is the author's basic information. And each update of user_info updates the authtoken.

**Improvement**

Since Authtoken is now cached and would be on Production system, we can skip the user part of authtoken.
validate method of `agora_api/dbapi/components/client_api.py` can fetch the user Just-in-time of returning the validated user.

Benefits:
    
    * Do not need to support authtoken as a transaction_id in `agora_api/db/transactions.py`.
    * Do not need to loop through the authtoken collection and update user information.


Search Queries
--------------

`agora_api/dbapi/components/search_api.py` is where API handles resources that support the Like parameter.
It works in a bunch of modes:

    * If _ids are provided in request_params it will paginate through those.
    * In case a like parameter is provided. API speaks with Party and asks for ALL eligible ids that match the provided string. The List of ids is saved corresponding to a search_id. Any subsequent calls on the same handler can be made using the search_id. search_ids have a TTL of 10 miniutes.
    * If neither of _ids/like is provided it uses the optional feed_func parameter to generate the list of ids that can be accessed.

**TimeOut**

In case of a search time-out, API raises a 504 error with String "Search Timeout. Resend params."
Consumer is expected to re-send the query without the search_id and API would return with a new search_id valid for another 10 minutes.

**Improvement**

ids generated by feed_func need to be saved in a cache. Subsequent pagination hits should carry the previously saved search_id.
This will really bolster the "Load more" queries.

Invalidation
------------

Easiest way to queue ids that need to be invalidated is using
`handler.dbapi.simple.add_transaction(model_id(s))`

**NOTE**

id(s) are invalidated at the end of each request.
One **Cannot** invalidate while the request is being served.
Actual invalidation is carried out by `shelve_tasks` in `agora_api/kernel/base.py`

Stages of invalidation:

    1. Find topic_ids {Bundle,Siminar,Session} to be invalidated.
    2. Find authtokens to be updated(described above). This is not an invalidation.
    3. Find user ids to be invalidated. This is slightly expensive as ALL currently active authtokens need to be fetched and invalidated using a redis-pipe.

**Usher**

Since all of the subscriber/publisher/can_view logic has been moved, Invalidations also need to be collected from Usher.
`make_return` of `dbapi/components/subpub.py` expects a key `invalidations` in return data as an array of id(s) that need to be invalidated.
Each subpub call' return is formatted using make_return & any other illegal invocation will result in invalidations/tasks not being captured.


Problems
--------

    * Expecting an array of ids to be invalidated from subpub will work as long as Usher does not use goRoutines.
    * It would be easy to accept ids on a channel and send them as a batch, but improbable to send those with the request's return data.
    * Since redis is a key-value pair and not a tree-cache topic_id has to be stored as a key with value as a dictionary.
    * Each user_id has a corresponding json_dump of the topic stored. This design restricts TTL expiration for each user_id.
    * There are other alternatives like gibson (http://gibson-db.in/) which could be used, but thats another component to take care of.

Invalidation Handler
--------------------

API exposes an invalidate handler which is responsible for invalidating cache.

/invalidate

    * METHOD: POST
    * Input: key=_id(s) OR {key=KEY & subkey=_id(s)}
    * Output: True

Invalidation in shelve_tasks to support an array of tuples.

The final invalidations array could look like

```
invalidations = [
    "138097802129319459855057",
    "138079102129319459855057",
    "138069174058727688663702",
    "129328102413829737863702"
]
```

If the item is a tuple it should be read as tuple{key, subkeys}. Example

```
invalidations = [
    "138097802129319459855057",
    "138079102129319459855057",
    ("138155995814243924418098", ["138069174058727688663702", "129328102413829737863702"])
]
```

Also the consumers that filter out potential_ids should be changed accordingly.

```
        topic_ids = filter(
            lambda _id: _id.endswith(("18098", "47020", "62579")),
            transactions)

        for _id in topic_ids:
            self.bouncer.delete_topic_ident(_id)

```

Should be changed to

```
        for _id in transactions:
            if isinstance(_id, basestring):
                if _id.endswith(("18098", "47020", "62579")):
                    self.bouncer.delete_topic_ident(_id)

            elif isinstance(_id, tuple):
                if _id[0].endswith(("18098", "47020", "62579")):
                    self.bouncer.delete_topic_ident(_id[0], _id[1])
```

This would take care of the subkeys being deleted.
The other two consumers need not be altered since only Topic Cache has subkeys.

Invalidating Session Based Siminars
-----------------------------------

Party has a task called *SessionCron* defined in tasks.py

Each task in party can expose a classmethod add_to_queue of format:
```
    @classmethod
    def add_to_queue(cls, data, timestamp=None):
        pass
```

Role of add_to_queue is to perform pre-insert checks.
Example: If you want to keep a task as Singleton and ensure it is not inserted again your add_to_queue will look something like this.

```
class SampleTask(TaskBase):

    @classmethod
    def add_to_queue(cls, data, timestamp=None):
        unique_id = data.get("unique_id")
        existing = cls.tasks.find_one({
            'party': cls.__name__,
            "done": {"$ne": True},
            "data.unique_id": unique_id
        })

        if not existing:
            super(SampleTask, cls).add_to_queue(data, timestamp)
        else:
            print "Will not insert duplicate"
```

Use similar technique to ensure that you have one SessionCron for every Session.

Things to consider:

    * Ensure that a Task exists for each Session, just spawn it on the time of creation/editing. add_to_queue should handle the redundancy logic.
    * Logics like session is_unlaunched, has_not_started, is_deleted should be handled in the run method of the task.
    * This defering of computation will save you syncronization maintenance headache.

The run method of the Task currently looks like this:

```
    def run(self):
        siminar_ids = []
        for step in self.step_to_publish():
            if step['siminar_id'] not in siminar_ids:
                siminar_ids.append(step['siminar_id'])

        try:
            pipe.send()
        except Exception:
            pass
```

Since all we have to do is invalidate the cache. Alter this method to do something like this

```
    def run(self):
        session = get_session()
        if session is_unlaunched or has_not_started or is_deleted:
            #Mark this task as done. add_to_queue will spawn a new task if needed.
            return

        siminar_ids = []
        for step in self.step_to_publish():
            if step['siminar_id'] not in siminar_ids:
                siminar_ids.append(step['siminar_id'])

        for each siminar_id:
            api.send("/invalidate", {"key": siminar_id, "subkey": [siminar_students]})

```

